{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88625fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c514157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7dbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful\n",
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)\n",
    "\n",
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d589c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################\n",
    "# From baseline code\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcab8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a negative set\n",
    "\n",
    "userSet = set()\n",
    "bookSet = set()\n",
    "readSet = set()\n",
    "\n",
    "for u,b,r in allRatings:\n",
    "    userSet.add(u)\n",
    "    bookSet.add(b)\n",
    "    readSet.add((u,b))\n",
    "\n",
    "lUserSet = list(userSet)\n",
    "lBookSet = list(bookSet)\n",
    "\n",
    "notRead = set()\n",
    "# For each (user,book) entry in the validation set, sample a\n",
    "# negative entry by randomly choosing a book that user hasn’t read\n",
    "for u,b,r in ratingsValid:\n",
    "    b = random.choice(lBookSet)\n",
    "    while ((u,b) in readSet or (u,b) in notRead):\n",
    "        b = random.choice(lBookSet)\n",
    "    notRead.add((u,b))\n",
    "\n",
    "readValid = set()\n",
    "for u,b,r in ratingsValid:\n",
    "    readValid.add((u,b))\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer/denom\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ff289538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7485"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ratingsPerUser[u].append((b,r))\n",
    "# # ratingsPerItem[b].append((u,r))\n",
    "# # for each sample entry, get the books read by that user u\n",
    "# # for each user u2 that has rated the same book,\n",
    "#     # calculate the similarity: between the books u has rated and\n",
    "#     # the books that u1 has rated\n",
    "#     # the idea here is that if user u and u1 tend to read the same books,\n",
    "#     # then u will also read b\n",
    "#     # if there is sufficient similarity, decide that user u will read b\n",
    "\n",
    "# correct = 0\n",
    "# # make predictions on the validation set XValid and compare to yvalid\n",
    "# for (label,sample) in [(1, readValid), (0, notRead)]:\n",
    "#     for (u,b) in sample:\n",
    "#         maxItemSim = 0\n",
    "#         avgItemRating = 0\n",
    "#         books = set(ratingsPerUser[u])\n",
    "#         # Jaccard item-to-item similarity\n",
    "#         for u2,r in ratingsPerItem[b]:\n",
    "#             sim = Jaccard(books, set(ratingsPerUser[u2]))\n",
    "#             if sim > maxItemSim:\n",
    "#                 maxItemSim = sim\n",
    "#             avgItemRating += r\n",
    "        \n",
    "#         avgItemRating = 0 if len(ratingsPerItem[b]) == 0 else avgItemRating/len(ratingsPerItem[b])\n",
    "        \n",
    "#         maxUserSim = 0\n",
    "#         avgUserRating = 0\n",
    "#         users = set(ratingsPerItem[b])\n",
    "#         # Jaccard user-to-user similarity\n",
    "#         for b2,r in ratingsPerUser[u]:\n",
    "#             sim = Jaccard(users,set(ratingsPerItem[b2]))\n",
    "#             if sim > maxUserSim:\n",
    "#                 maxUserSim = sim\n",
    "#                 avgUserRating += r\n",
    "#         avgUserRating = 0 if len(ratingsPerUser[u]) == 0 else avgUserRating/len(ratingsPerUser[u])\n",
    "                \n",
    "#         pred = 0\n",
    "#         # prediction based on Jaccard threshold and popularity threshold\n",
    "#         if maxUserSim > 0.013 or maxItemSim > 0.2 or len(ratingsPerItem[b]) > 40:\n",
    "#             pred = 1\n",
    "#         # if avgItemRating is above a threshold\n",
    "#         # if avgUserRating is within range [avgItemRating-.5, avgItemRating+.5]\n",
    "#         if avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20:\n",
    "#             pred = 1\n",
    "#         if avgItemRating-.3 <= avgUserRating <= avgItemRating+.3:\n",
    "#             pred = 1\n",
    "#         if pred == label:\n",
    "#             correct += 1\n",
    "# correct / (len(readValid) + len(notRead))\n",
    "# # 0.74820 for no maxItemSim\n",
    "# # 0.73210 for maxItemSim > 0.013\n",
    "# # 0.74190 for maxItemSim > 0.053\n",
    "# # 0.74395 for maxItemSim > 0.063\n",
    "# # 0.74585 for maxItemSim > 0.073\n",
    "# # 0.74650 for maxItemSim > 0.083\n",
    "# # 0.74755 for maxItemSim > 0.1\n",
    "# # 0.74820 for maxItemSim > 0.15\n",
    "# # 0.74830 for maxItemSim > 0.2\n",
    "# # 0.74820 for maxItemSim > 0.25\n",
    "\n",
    "# # 0.74805 avgItemRating-.5 <= avgUserRating <= avgItemRating+.5, maxItemSim > 0.25\n",
    "# # 0.74785 avgItemRating-.8 <= avgUserRating <= avgItemRating+.8, maxItemSim > 0.25\n",
    "# # 0.74830 avgItemRating-.4 <= avgUserRating <= avgItemRating+.4, maxItemSim > 0.25\n",
    "# # 0.74840 avgItemRating-.4 <= avgUserRating <= avgItemRating+.4, maxItemSim > 0.2\n",
    "# # 0.74830 avgItemRating-.3 <= avgUserRating <= avgItemRating+.3, maxItemSim > 0.2\n",
    "# # 0.74835 avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20, maxItemSim > 0.25\n",
    "# # 0.74840 avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20, maxItemSim > 0.2\n",
    "# # 0.7485 avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20, avgItemRating-.4 <= avgUserRating <= avgItemRating+.4:\n",
    "# # 0.7485 avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20, avgItemRating-.3 <= avgUserRating <= avgItemRating+.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "974b2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    maxItemSim = 0\n",
    "    avgItemRating = 0\n",
    "    books = set(ratingsPerUser[u])\n",
    "    # Jaccard item-to-item similarity\n",
    "    for u2,r in ratingsPerItem[b]:\n",
    "        sim = Jaccard(books, set(ratingsPerUser[u2]))\n",
    "        if sim > maxItemSim:\n",
    "            maxItemSim = sim\n",
    "        avgItemRating += r\n",
    "    avgItemRating = 0 if len(ratingsPerItem[b]) == 0 else avgItemRating/len(ratingsPerItem[b])\n",
    "    \n",
    "    maxUserSim = 0\n",
    "    avgUserRating = 0\n",
    "    users = set(ratingsPerItem[b])\n",
    "    # Jaccard user-to-user similarity\n",
    "    for b2,r in ratingsPerUser[u]:\n",
    "        sim = Jaccard(users,set(ratingsPerItem[b2]))\n",
    "        if sim > maxUserSim:\n",
    "            maxUserSim = sim\n",
    "            avgUserRating += r\n",
    "    avgUserRating = 0 if len(ratingsPerUser[u]) == 0 else avgUserRating/len(ratingsPerUser[u])\n",
    "    \n",
    "    pred = 0\n",
    "    if maxUserSim > 0.02 or maxItemSim > 0.2 or len(ratingsPerItem[b]) > 35:\n",
    "        pred = 1\n",
    "#     # if avgItemRating is above a threshold \n",
    "#     # and the item has a sufficient number of ratings to source a reliable average\n",
    "#     if avgItemRating > 3.5 and len(ratingsPerItem[b]) > 20:\n",
    "#         pred = 1\n",
    "#     # if avgUserRating is within range [avgItemRating-.3, avgItemRating+.3]\n",
    "#     if avgItemRating-.3 <= avgUserRating <= avgItemRating+.3:\n",
    "#         pred = 1\n",
    "        \n",
    "    _ = predictions.write(u + ',' + b + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "# 0.7683 len(ratingsPerItem[b]) > 35\n",
    "# 0.7648 baseline\n",
    "# 0.769 maxUserSim > 0.02 or maxItemSim > 0.2 or len(ratingsPerItem[b]) > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9d1a33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a negative set for test set and validation set\n",
    "\n",
    "# userSet = set()\n",
    "# bookSet = set()\n",
    "# readSet = set()\n",
    "\n",
    "# for u,b,r in allRatings:\n",
    "#     userSet.add(u)\n",
    "#     bookSet.add(b)\n",
    "#     readSet.add((u,b))\n",
    "\n",
    "# lUserSet = list(userSet)\n",
    "# lBookSet = list(bookSet)\n",
    "\n",
    "# notReadAll = set()\n",
    "# # For each (user,book) entry in the validation set, sample a\n",
    "# # negative entry by randomly choosing a book that user hasn’t read\n",
    "# for u,b,r in ratingsValid:\n",
    "#     b = random.choice(lBookSet)\n",
    "#     while ((u,b) in readSet or (u,b) in notReadAll):\n",
    "#         b = random.choice(lBookSet)\n",
    "#     notReadAll.add((u,b))\n",
    "\n",
    "# readValid = set()\n",
    "# for u,b,r in ratingsValid:\n",
    "#     readValid.add((u,b))\n",
    "    \n",
    "# # add negative set to validation set\n",
    "# Xvalid = list(readValid) + list(notReadAll)\n",
    "# yvalid = [1]*len(readValid) + [0]*len(notReadAll)\n",
    "# # print(Xvalid[len(ratingsValid):len(ratingsValid)+5])\n",
    "# # print(yvalid[len(ratingsValid):len(ratingsValid)+5])\n",
    "\n",
    "# notReadTrain = set()\n",
    "# for u,b,r in ratingsTrain:\n",
    "#     b = random.choice(lBookSet)\n",
    "#     while ((u,b) in readSet or (u,b) in notReadAll):\n",
    "#         b = random.choice(lBookSet)\n",
    "#     notReadAll.add((u,b))\n",
    "#     notReadTrain.add((u,b))\n",
    "    \n",
    "# readTrain = set()\n",
    "# for u,b,r in ratingsTrain:\n",
    "#     readTrain.add((u,b))\n",
    "    \n",
    "# # add negative sets to corresponding sets\n",
    "# Xtrain = list(readTrain) + list(notReadTrain)\n",
    "# ytrain = [1]*len(readTrain) + [0]*len(notReadTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0ca71a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7277"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def feature(datum):\n",
    "#     u, b = datum\n",
    "# #     maxItemSim = 0\n",
    "# #     minItemSim = 1\n",
    "# #     books = set(ratingsPerUser[u])\n",
    "# #     # Jaccard item-to-item similarity\n",
    "# #     for u2,r in ratingsPerItem[b]:\n",
    "# #         if u2 == u:\n",
    "# #             continue\n",
    "# #         sim = Jaccard(books, set(ratingsPerUser[u2]))\n",
    "# #         if sim > maxItemSim:\n",
    "# #             maxItemSim = sim\n",
    "# # #         if sim < minItemSim:\n",
    "# # #             minItemSim = sim\n",
    "#     avgItemRating = 0\n",
    "#     for u2,r in ratingsPerItem[b]:\n",
    "#         avgItemRating += r\n",
    "        \n",
    "#     avgItemRating = 0 if len(ratingsPerItem[b]) == 0 else avgItemRating/len(ratingsPerItem[b])\n",
    "    \n",
    "#     maxUserSim = 0\n",
    "# #     minUserSim = 1\n",
    "#     avgUserRating = 0\n",
    "#     users = set(ratingsPerItem[b])\n",
    "#     # Jaccard user-to-user similarity\n",
    "#     for b2,r in ratingsPerUser[u]:\n",
    "#         if b2 == b:\n",
    "#             continue\n",
    "#         sim = Jaccard(users,set(ratingsPerItem[b2]))\n",
    "#         if sim > maxUserSim:\n",
    "#             maxUserSim = sim\n",
    "# #         if sim < minUserSim:\n",
    "# #             minUserSim = sim\n",
    "#         avgUserRating += r\n",
    "#     avgUserRating = 0 if len(ratingsPerUser[u]) == 0 else avgUserRating/len(ratingsPerUser[u])\n",
    "\n",
    "#     bookPopularity = len(ratingsPerItem[b])\n",
    "#     avgRating = 0    \n",
    "#     feat = [1]\n",
    "#     feat.append(bookPopularity)\n",
    "#     feat.append(avgItemRating)\n",
    "# #     feat.append(avgUserRating)\n",
    "# #     feat.append(maxItemSim)\n",
    "#     feat.append(maxUserSim)\n",
    "#     return feat\n",
    "\n",
    "# # X = [feature(d) for d in data]\n",
    "# # y = [1]*len(X)\n",
    "# fitXtrain = [feature(d) for d in Xtrain]\n",
    "# mod = linear_model.LogisticRegression(C=10**(-3))\n",
    "# mod.fit(fitXtrain, ytrain)\n",
    "# fitXvalid = [feature(d) for d in Xvalid]\n",
    "# pred = mod.predict(fitXvalid)\n",
    "# correct = pred == yvalid\n",
    "# sum(correct) / len(correct)\n",
    "# # 0.7097 class_weight='balanced' pop, maxUserSim \n",
    "# # 0.65235 class_weight='balanced' pop, maxUserSim, maxItemSim \n",
    "# # 0.65235 class_weight='balanced' pop, maxUserSim, maxItemSim, minUser/Item \n",
    "# # adding min doesnt change anything since its always 0\n",
    "# # 0.6559 class_weight='balanced' pop, maxItemSim \n",
    "# # 0.63225 class_weight='balanced' pop, maxUserSim, maxItemSim avgItem/UserSim \n",
    "# # 0.72595 C=10**(-4), class_weight='balanced' pop, maxItem, maxUser \n",
    "# # 0.72715 C=10**(-4), class_weight='balanced' pop, maxUser\n",
    "\n",
    "# # 0.72725 C=10**(-3), class_weight='balanced' pop, maxUser\n",
    "# # 0.72725 C=10**(-3) pop, maxUser\n",
    "# # 0.72770 C=10**(-3) pop, maxUser, avgItemRating\n",
    "# # 0.7048 C=10**(1) pop, maxUser\n",
    "# # 0.7041 C=10**(2) pop, maxUser\n",
    "# # 0.70375 C=10**(3) pop, maxUser\n",
    "# # 0.72675 C=10**(-3) pop, maxUser, avgItemRating\n",
    "# # 0.68685 C=10**(-3) pop, avgItem/UserRating, maxUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "968a4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = mod.coef_\n",
    "# theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b04b2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest = []\n",
    "# predictions = open(\"predictions_Read.csv\", 'w')\n",
    "# for l in open(\"pairs_Read.csv\"):\n",
    "#     if l.startswith(\"userID\"):\n",
    "#         predictions.write(l)\n",
    "#         continue\n",
    "#     u,b = l.strip().split(',')\n",
    "#     Xtest.append((u,b))\n",
    "    \n",
    "# fitXtest = [feature(d) for d in Xtest]\n",
    "# predict = mod.predict(fitXtest)\n",
    "# for i in range(len(predict)):\n",
    "#     u,b = Xtest[i]\n",
    "#     pred = predict[i]\n",
    "#     _ = predictions.write(u + ',' + b + ',' + str(pred) + '\\n')\n",
    "\n",
    "# predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bde4a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = list(zip([1]*len(readValid), readValid)) + list(zip([0]*len(notRead), notRead))\n",
    "# print((data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b83a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Category prediction (CSE158 only)              #\n",
    "##################################################\n",
    "data = []\n",
    "\n",
    "for d in readGz(\"train_Category.json.gz\"):\n",
    "    data.append(d)\n",
    "# data[0]\n",
    "# {'user_id': 'u75242413',\n",
    "#  'review_id': 'r45843137',\n",
    "#  'rating': 4,\n",
    "#  'review_text': \"a clever book with a deeply troubling premise and an intriguing protagonist. Thompson's clean, sparse prose style kept each page feeling light even as some rather heavy existential questions dropped upon them. I enjoyed it. \\n and that cover design is boom-pow gorgeous.\",\n",
    "#  'n_votes': 1,\n",
    "#  'genre': 'mystery_thriller_crime',\n",
    "#  'genreID': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60c628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genresPerUser = {}\n",
    "# avgGenresRatingPerUser = {}\n",
    "# globalGenreAvg = [0, 0, 0, 0, 0]\n",
    "# totalReviews = [0, 0, 0, 0, 0]\n",
    "for d in data:\n",
    "    u = d['user_id']\n",
    "    gid = d['genreID']\n",
    "    if u not in genresPerUser:\n",
    "        genresPerUser[u] = [0]*5\n",
    "#     if u not in avgGenresRatingPerUser:\n",
    "#         avgGenresRatingPerUser[u] = [0]*5\n",
    "    genresPerUser[u][gid] += 1\n",
    "#     avgGenresRatingPerUser[u][gid] += d['rating']\n",
    "#     globalGenreAvg[gid] += d['rating']\n",
    "#     totalReviews[gid] += 1\n",
    "# # compute user averages\n",
    "# for u in genresPerUser.keys():\n",
    "#     for i in range(0,5):\n",
    "#         if genresPerUser[u][i] == 0:\n",
    "#             avgGenresRatingPerUser[u][i] = 0\n",
    "#         else:\n",
    "#             avgGenresRatingPerUser[u][i] /= genresPerUser[u][i]\n",
    "# # compute global averages            \n",
    "# for i in range(0,5):\n",
    "#     if totalReviews[i] == 0:\n",
    "#         globalGenreAvg[i] = 0\n",
    "#     else:\n",
    "#         globalGenreAvg[i] /= totalReviews[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d9d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w not in stop_words:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "# counts[:10]\n",
    "# [(739469, 'the'),\n",
    "#  (447352, 'and'),\n",
    "#  (393557, 'a'),\n",
    "#  (373089, 'to'),\n",
    "#  (364764, 'i'),\n",
    "#  (324675, 'of'),\n",
    "#  (218882, 'is'),\n",
    "#  (212488, 'in'),\n",
    "#  (204960, 'it'),\n",
    "#  (193791, 'this')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d47d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature1(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    u = datum['user_id']\n",
    "    if u in genresPerUser:\n",
    "        feat += (genresPerUser[u])\n",
    "#         feat += avgGenresRatingPerUser[u]\n",
    "    else:\n",
    "        feat += [0,0,0,0,0]\n",
    "        # replace with global average rating for each genre instead of 0\n",
    "#         feat += globalGenreAvg\n",
    "#         feat += [0,0,0,0,0]\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "\n",
    "# userGenreCount: [_,_,_,_,_] num of ratings for each genre\n",
    "# avgUserGenreRating: [_,_,_,_,_] user's avg rating for books in each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53658f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\davj5\\miniconda3\\envs\\mae146\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8336"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question 8\n",
    "\n",
    "NW = 15000 # dictionary size\n",
    "words = [x[1] for x in counts[:NW]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "X = [feature1(d) for d in data]\n",
    "y = [d['genreID'] for d in data]\n",
    "\n",
    "Xtrain = X[:9*len(X)//10]\n",
    "ytrain = y[:9*len(y)//10]\n",
    "Xvalid = X[9*len(X)//10:]\n",
    "yvalid = y[9*len(y)//10:]\n",
    "\n",
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, ytrain)\n",
    "\n",
    "pred = mod.predict(Xvalid)\n",
    "correct = pred == yvalid\n",
    "sum(correct) / len(correct)\n",
    "# 0.5093 C=10**(-4)\n",
    "# 0.6324 C=10**(-3)\n",
    "# 0.6792 C=10**(-2)\n",
    "# 0.6851 C=10**(-1)\n",
    "# 0.6907 C=1\n",
    "# 0.6862 C=10**(1)\n",
    "# 0.6883 C=10**(2)\n",
    "# 0.685 C=10**(3)\n",
    "# 0.6886 C=10**(4)\n",
    "# 0.823 with genre count, c=1, nw =2000\n",
    "# 0.8259 with genre count, c=1, nw =1000 but poor on leaderboard\n",
    "# 0.8242, 0.7088 leaderboard: genrecount, nw=3000, c=1\n",
    "# 0.8738, 0.6714 leaderboard: genrecount avgrating nw=3000, c=1\n",
    "# 0.7122 leaderboard: genrecount nw=4000, c=1\n",
    "# 0.7148 leaderboard genrecount nw=5000, c=1\n",
    "# 0.6634 leaderboard genrecount globalavg nw=5000, c=1\n",
    "# 0.7194 leaderboard genrecount nw=7000, c=1\n",
    "# 0.7314 leaderboard genrecount nw=10000, c=1\n",
    "# 0.7398 leaderboard genrecount nw=15000, c=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091ea7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on test set\n",
    "data_test = []\n",
    "\n",
    "for d in readGz(\"test_Category.json.gz\"):\n",
    "    data_test.append(d)\n",
    "Xtest = [feature1(d) for d in data_test]\n",
    "pred_test = mod.predict(Xtest)\n",
    "predictions = open(\"predictions_Category.csv\", 'w')\n",
    "pos = 0\n",
    "\n",
    "for l in open(\"pairs_Category.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    _ = predictions.write(u + ',' + b + ',' + str(pred_test[pos]) + '\\n')\n",
    "    pos += 1\n",
    "\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
